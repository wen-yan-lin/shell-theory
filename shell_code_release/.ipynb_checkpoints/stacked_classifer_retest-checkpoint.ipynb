{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/daniel/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/daniel/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/daniel/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/daniel/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/daniel/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/daniel/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/daniel/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/daniel/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/daniel/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/daniel/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/daniel/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/daniel/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import dirUtil \n",
    "from highDimLearning import vgg2feat\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os\n",
    "import random \n",
    "    \n",
    "\n",
    "def feat2labels(feat):\n",
    "    numClass = len(feat)\n",
    "    numDim = feat[0].shape[1]\n",
    "    numPts = 0\n",
    "    for i in range(numClass):\n",
    "        numPts = numPts + feat[i].shape[0]\n",
    "\n",
    "    allFeat = np.zeros([numPts, numDim])\n",
    "    allLabels = np.zeros(numPts, dtype = int)\n",
    "    cur = 0\n",
    "    for i in range(numClass):\n",
    "        allFeat[cur:cur+feat[i].shape[0],:] = feat[i]\n",
    "        allLabels[cur:cur+feat[i].shape[0]] = i\n",
    "        cur = cur + feat[i].shape[0]\n",
    "    return allFeat, allLabels\n",
    "        \n",
    "    \n",
    "def readNetVlad(folder, numDim =4096):\n",
    "    folderName = os.path.basename(os.path.normpath(folder))\n",
    "    print(folderName)\n",
    "    file = folder +'/vd16_pitts30k_conv5_3_vlad_preL2_intra_white_' +folderName + '_db.bin'    \n",
    "    data = np.fromfile(file, '<f4')\n",
    "    numPts = int(len(data)/numDim)\n",
    "    return np.reshape(data, [numPts, numDim])\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def cluster_acc(Y_pred, Y):\n",
    "    assert Y_pred.size == Y.size\n",
    "    D = max(Y_pred.max(), Y.max())+1\n",
    "    w = np.zeros((D,D), dtype=np.int64)\n",
    "    for i in range(Y_pred.size):\n",
    "        w[Y_pred[i], Y[i]] += 1\n",
    "    \n",
    "    row_ind, col_ind = linear_sum_assignment(w.max() - w)\n",
    "    \n",
    "    print(w[row_ind,col_ind])\n",
    "    return w[row_ind,col_ind].sum()/Y_pred.size, w\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9793977d1973>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# trainingNum = 1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/media/daniel/D/Data/basicMITData/abbey_small/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# feat1 = readNetVlad('/media/daniel/D/Data/basicMITData/abbey')\n",
    "    \n",
    "# feat2 = readNetVlad('/media/daniel/D/Data/basicMITData/airport_terminal')\n",
    "\n",
    "# feat3 = readNetVlad('/media/sliu/New Volume/basicMITData/alley')\n",
    "\n",
    "# feat4 = readNetVlad('/media/sliu/New Volume/basicMITData/amphitheater')\n",
    "\n",
    "\n",
    "# trainingNum = 1000\n",
    "\n",
    "tf.reset_default_graph()\n",
    "K.clear_session()\n",
    "folder = '/media/daniel/D/Data/basicMITData/abbey_small/'\n",
    "feats = dirUtil.dir2vgg16(folder)\n",
    "feat1 = vgg2feat(feats)\n",
    "\n",
    "\n",
    "folder2 = '/media/daniel/D/Data/basicMITData/airport_small/'\n",
    "tf.reset_default_graph()\n",
    "K.clear_session()\n",
    "feats = dirUtil.dir2vgg16(folder2)\n",
    "feat2 = vgg2feat(feats)\n",
    "\n",
    "# folder3 = '/home/sliu/code/keras-tutorial/animals/panda/'\n",
    "# tf.reset_default_graph()\n",
    "# K.clear_session()\n",
    "# feats = dirUtil.dir2vgg16(folder3)\n",
    "# feat3 = vgg2feat(feats)\n",
    "\n",
    "feat, gt = feat2labels([feat1, feat2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "feat = np.load('/media/daniel/D/Data/STL-10/feats_2048.npy')\n",
    "gt = np.load('/media/daniel/D/Data/STL-10/labels_2048.npy')-1\n",
    "\n",
    "mask = gt>-1#np.logical_or(gt==3, gt==4)\n",
    "feat = feat[mask,:]\n",
    "gt = gt[mask]\n",
    "\n",
    "#feat = feat-np.mean(feat)\n",
    "\n",
    "feat = feat#/np.linalg.norm(feat, axis =1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "feat = np.reshape(x_train, [x_train.shape[0], x_train.shape[1]*x_train.shape[2]])\n",
    "feat = feat#/np.linalg.norm(feat, axis =1, keepdims=True)\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "\n",
    "# pca = PCA(n_components=0.9)\n",
    "# pca.fit(feat)\n",
    "# feat = pca.transform(feat)\n",
    "\n",
    "\n",
    "gt = y_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "feat = np.reshape(x_test, [x_test.shape[0], x_test.shape[1]*x_test.shape[2]])\n",
    "#feat = feat/np.linalg.norm(feat, axis =1, keepdims=True)\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "# feat = scaler.fit_transform(feat)\n",
    "# pca = PCA(n_components=0.9)\n",
    "# feat = pca.fit_transform(feat)\n",
    "\n",
    "\n",
    "\n",
    "gt = y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def get_train_test_feat(feat, target_class, num_train, gt):\n",
    "    mask_ = gt == target_class[0]\n",
    "    for i in target_class:\n",
    "        mask_ = mask_ + (gt ==i)\n",
    "    ind = np.where(mask_)[0]\n",
    "    ind = np.random.choice(ind, num_train, replace=False)\n",
    "    train_feat = copy.copy(feat[ind,:])\n",
    "    mask = np.ones(gt.size, dtype=bool)\n",
    "    mask[ind] = 0\n",
    "    test_feat = copy.copy(feat[mask, :])\n",
    "    test_gt = gt[mask]\n",
    "    test_gt_bool = np.zeros(test_gt.size, dtype=bool)\n",
    "    for i in target_class:\n",
    "        test_gt_bool[test_gt == i] = True\n",
    "\n",
    "    \n",
    "    return train_feat, test_feat, test_gt_bool\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_means(feat, gt):\n",
    "    num_clus = int(np.max(gt)+1)\n",
    "    means = np.zeros([num_clus, feat.shape[1]])\n",
    "    for i in range(num_clus):\n",
    "        means[i] = np.mean(feat[gt==i,:], axis =0)\n",
    "    return means\n",
    "\n",
    "def sorted_neighbors_of_i(m_all, i):\n",
    "    neighbors = np.zeros(m_all.shape[0])\n",
    "    for j in range(m_all.shape[0]):\n",
    "        neighbors[j] = np.linalg.norm(m_all[i,:]-m_all[j,:])\n",
    "    return neighbors, np.argsort(neighbors)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9232727564102563\n",
      "0.9691747863247865\n",
      "0.9808043803418804\n"
     ]
    }
   ],
   "source": [
    "def stacked_classifer(train_feat, gt, target, m_all):\n",
    "    _, neighs = sorted_neighbors_of_i(m_all, target)\n",
    "    classifers = []\n",
    "    current_shell = []\n",
    "    current_shell.append(target)\n",
    "    for i in neighs:\n",
    "        if not i == target:\n",
    "            current_shell.append(i)\n",
    "        if len(current_shell)> 1:\n",
    "            m1 = np.mean(m_all[current_shell,:], axis =0, keepdims=True)\n",
    "            #m1 = np.mean(m_all[[target, i],:], axis =0, keepdims=True)\n",
    "            tf = train_feat-m1\n",
    "            tf = tf/np.linalg.norm(tf, axis =1, keepdims=True)\n",
    "            clf1 = OneClassSVM(gamma='auto').fit(tf)\n",
    "            classifers.append({'mean': m1, 'classifer': clf1})\n",
    "\n",
    "    return classifers\n",
    "\n",
    "\n",
    "def score_samples(classifers, test_feat):\n",
    "    score = np.zeros([test_feat.shape[0], len(classifers)])\n",
    "    for i in range(len(classifers)):\n",
    "        m = classifers[i]['mean']\n",
    "        tsf = test_feat-m\n",
    "        tsf = tsf/np.linalg.norm(tsf, keepdims=True, axis =1)\n",
    "        s = classifers[i]['classifer'].score_samples(tsf)   \n",
    "        score[:,i] = s\n",
    "    return score\n",
    "\n",
    "target = 5\n",
    "num_train = 500\n",
    "\n",
    "feat_ = feat \n",
    "feat_ = feat_/np.linalg.norm(feat_, axis =1, keepdims=True)\n",
    "\n",
    "m_all = get_means(feat_, gt)\n",
    "train_feat, test_feat, test_gt = get_train_test_feat(feat_, [target], num_train, gt)\n",
    "#mask = np.logical_or(gt==3, gt==5)\n",
    "#train_feat, test_feat, test_gt = get_train_test_feat(feat[mask,:], [3], num_train, gt[mask])\n",
    "\n",
    "\n",
    "classifers = stacked_classifer(train_feat, gt, target, m_all)\n",
    "s = score_samples(classifers, test_feat)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(test_gt, s[:,0])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(roc_auc)\n",
    "fpr, tpr, _ = roc_curve(test_gt, s[:,-1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(roc_auc)\n",
    "fpr, tpr, _ = roc_curve(test_gt, np.mean(s, axis =1))\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "(12500, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.755"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = np.argsort(-np.mean(s, axis =1))\n",
    "print(test_gt.size)\n",
    "print(s.shape)\n",
    "np.mean(test_gt[ind[:1300-num_train]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from highDimLearningX import estModel22\n",
    "from highDimLearningX import estModel23\n",
    "\n",
    "target = 0\n",
    "num_train = 500\n",
    "\n",
    "feat_ = feat \n",
    "feat_ = feat_/np.linalg.norm(feat_, axis =1, keepdims=True)\n",
    "\n",
    "m_all = get_means(feat_, gt)\n",
    "train_feat, test_feat, test_gt = get_train_test_feat(feat_, [target], num_train, gt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def get_means_fancy(feat, gt):\n",
    "    num_clus = int(np.max(gt)+1)\n",
    "    means = np.zeros([num_clus, feat.shape[1]])\n",
    "    for i in range(num_clus):\n",
    "        f = feat[gt==i,:]\n",
    "        means[i] = np.mean(f[:10,:], axis=0)\n",
    "        #kmeans = KMeans(n_clusters=300, random_state=0).fit(feat[gt==i,:])\n",
    "        #means[i] = np.mean(kmeans.cluster_centers_, axis =0)\n",
    "    return means\n",
    "\n",
    "m_all = get_means(feat_, gt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2048)\n"
     ]
    }
   ],
   "source": [
    "print(m_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "_, neighs = sorted_neighbors_of_i(m_all, target)\n",
    "\n",
    "new_m = []\n",
    "current_shell = [target]\n",
    "for i in neighs:\n",
    "    if not i == target:\n",
    "        current_shell.append(i)\n",
    "    if len(current_shell)> 1:\n",
    "        m1 = np.mean(m_all[current_shell,:], axis =0, keepdims=True)\n",
    "        new_m.append(m1)\n",
    "\n",
    "# for i in range(len(new_m)-1):\n",
    "#     new_m[i] = new_m[i]*0.8 + new_m[i+1]*0.2\n",
    "    \n",
    "def stacked_classifer_newM(train_feat, new_m):\n",
    "    classifers = []\n",
    "    for m in new_m:\n",
    "        tf = train_feat-np.squeeze(m)\n",
    "        tf = tf/np.linalg.norm(tf, axis =1, keepdims=True)\n",
    "        #classifers.append({'mean': np.squeeze(m), 'classifer': np.mean(tf, axis =0, keepdims=True)})\n",
    "\n",
    "        clf1 = OneClassSVM(gamma='auto').fit(tf)\n",
    "        classifers.append({'mean': np.squeeze(m), 'classifer': clf1})\n",
    "\n",
    "    return classifers\n",
    "\n",
    "def score_samples_fancy(classifers, test_feat):\n",
    "    score = np.zeros([test_feat.shape[0], len(classifers)])\n",
    "    for i in range(len(classifers)):\n",
    "        m = classifers[i]['mean']\n",
    "        tsf = test_feat-m\n",
    "        tsf = tsf/np.linalg.norm(tsf, keepdims=True, axis =1)\n",
    "        s = pairwise_distances(classifers[i]['classifer'], tsf)\n",
    "        score[:,i] = s\n",
    "    return score\n",
    "\n",
    "\n",
    "classifers = stacked_classifer_newM(train_feat, new_m)\n",
    "s = score_samples(classifers, test_feat)\n",
    "#s = score_samples_fancy(classifers, test_feat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.825"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.neighbors.kde import KernelDensity\n",
    "\n",
    "s_train = score_samples_fancy(classifers, train_feat)\n",
    "s_test = score_samples_fancy(classifers, test_feat)\n",
    "\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=0.05).fit(s_train)\n",
    "s = kde.score_samples(s_test)\n",
    "ind = np.argsort(-s)\n",
    "np.mean(test_gt[ind[:1300-num_train]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "(12500, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.84625"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = np.argsort(np.mean(s, axis =1))\n",
    "print(test_gt.size)\n",
    "print(s.shape)\n",
    "np.mean(test_gt[ind[:1300-num_train]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from highDimLearningX import estModel22\n",
    "from highDimLearningX import estModel23\n",
    "\n",
    "_, neighs = sorted_neighbors_of_i(m_all, target)\n",
    "new_m = []\n",
    "current_shell = []\n",
    "for i in neighs:\n",
    "    current_shell.append(i)\n",
    "    if len(current_shell)> 1:\n",
    "        mask = np.zeros(feat_.shape[0], dtype=bool)\n",
    "        for j in current_shell:\n",
    "            mask[gt==j] = 1\n",
    "        cur_feat = feat_[mask,:]\n",
    "        \n",
    "        m = np.mean(cur_feat, axis =0, keepdims = True)\n",
    "        f = np.sum(np.square(train_feat -m), axis =1)\n",
    "        sig =  np.mean(f, keepdims=True)\n",
    "        \n",
    "        w, sig = estModel23(cur_feat, m, sig, numShells=1, weight=0.01)\n",
    "        new_m.append(w)\n",
    "\n",
    "        \n",
    "#         pca = PCA(n_components=1)\n",
    "#         pca.fit(cur_feat)\n",
    "#         m = pca.components_\n",
    "#         sig = np.mean(np.matmul(cur_feat, np.transpose(m)), axis=0)\n",
    "#         w, sig = estModel22(cur_feat, np.transpose(m), sig, numShells=1, weight=1.0)\n",
    "#         new_m.append(w)\n",
    "\n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_classifer_newM(train_feat, new_m):\n",
    "    classifers = []\n",
    "    for m in new_m:\n",
    "        tf = train_feat-np.squeeze(m)\n",
    "        tf = tf/np.linalg.norm(tf, axis =1, keepdims=True)\n",
    "        clf1 = OneClassSVM(gamma='auto').fit(tf)\n",
    "        classifers.append({'mean': np.squeeze(m), 'classifer': clf1})\n",
    "\n",
    "    return classifers\n",
    "\n",
    "classifers = stacked_classifer_newM(train_feat, new_m)\n",
    "s = score_samples(classifers, test_feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "(12500, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.58125"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = np.argsort(-np.mean(s, axis =1))\n",
    "print(test_gt.size)\n",
    "print(s.shape)\n",
    "np.mean(test_gt[ind[:1300-num_train]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
