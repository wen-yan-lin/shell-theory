{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "import dirUtil \n",
    "from highDimLearning import vgg2feat\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os\n",
    "import random \n",
    "    \n",
    "\n",
    "def feat2labels(feat):\n",
    "    numClass = len(feat)\n",
    "    numDim = feat[0].shape[1]\n",
    "    numPts = 0\n",
    "    for i in range(numClass):\n",
    "        numPts = numPts + feat[i].shape[0]\n",
    "\n",
    "    allFeat = np.zeros([numPts, numDim])\n",
    "    allLabels = np.zeros(numPts, dtype = int)\n",
    "    cur = 0\n",
    "    for i in range(numClass):\n",
    "        allFeat[cur:cur+feat[i].shape[0],:] = feat[i]\n",
    "        allLabels[cur:cur+feat[i].shape[0]] = i\n",
    "        cur = cur + feat[i].shape[0]\n",
    "    return allFeat, allLabels\n",
    "        \n",
    "    \n",
    "def readNetVlad(folder, numDim =4096):\n",
    "    folderName = os.path.basename(os.path.normpath(folder))\n",
    "    print(folderName)\n",
    "    file = folder +'/vd16_pitts30k_conv5_3_vlad_preL2_intra_white_' +folderName + '_db.bin'    \n",
    "    data = np.fromfile(file, '<f4')\n",
    "    numPts = int(len(data)/numDim)\n",
    "    return np.reshape(data, [numPts, numDim])\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def cluster_acc(Y_pred, Y):\n",
    "    assert Y_pred.size == Y.size\n",
    "    D = max(Y_pred.max(), Y.max())+1\n",
    "    w = np.zeros((D,D), dtype=np.int64)\n",
    "    for i in range(Y_pred.size):\n",
    "        w[Y_pred[i], Y[i]] += 1\n",
    "    \n",
    "    row_ind, col_ind = linear_sum_assignment(w.max() - w)\n",
    "    \n",
    "    print(w[row_ind,col_ind])\n",
    "    return w[row_ind,col_ind].sum()/Y_pred.size, w\n",
    "\n",
    "\n",
    "\n",
    "feat = np.load('/media/daniel/D/Data/STL-10/feats_2048.npy')\n",
    "gt = np.load('/media/daniel/D/Data/STL-10/labels_2048.npy')-1\n",
    "\n",
    "mask = gt>-1#np.logical_or(gt==3, gt==4)\n",
    "feat = feat[mask,:]\n",
    "gt = gt[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171.98021\n",
      "0.09262807\n",
      "0.13079253\n",
      "0.14231682\n",
      "0.12135451\n",
      "0.10030161\n",
      "0.1914751\n",
      "0.17036219\n",
      "0.18578367\n",
      "0.2989496\n",
      "0.15359566\n",
      "0.37783706\n",
      "0.24687095\n",
      "0.20078334\n",
      "0.21815717\n",
      "0.17809394\n",
      "0.27462497\n",
      "0.21294753\n",
      "0.20084304\n",
      "0.19530696\n",
      "0.03468439\n",
      "0.06671337\n",
      "0.06568428\n",
      "0.06474669\n",
      "0.06389156\n",
      "0.063110866\n",
      "0.062397815\n",
      "0.061745785\n",
      "0.06114951\n",
      "0.0606037\n",
      "0.06010372\n",
      "0.059645317\n",
      "0.0592249\n",
      "0.058839045\n",
      "0.058484357\n",
      "0.058158465\n",
      "0.05785861\n",
      "0.05758258\n",
      "0.0573282\n",
      "0.057093773\n",
      "0.4880769230769231 0.5688108875993334\n"
     ]
    }
   ],
   "source": [
    "from cluster_eval import eval_ours\n",
    "\n",
    "purity, map_, our_labels = eval_ours(feat, 10, gt)\n",
    "print(purity, map_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "0.8665384615384616 0.9708693700422686\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "from cluster_eval import eval_kmeans\n",
    "\n",
    "purity, map_, our_labels = eval_kmeans(feat, 2, gt)\n",
    "print(purity, map_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0829 10:25:48.761914 140393929348928 deprecation_wrapper.py:119] From /media/daniel/D/code/cnnTest2/clusteringXXX.py:161: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "W0829 10:26:11.471458 140393929348928 deprecation_wrapper.py:119] From /media/daniel/D/code/cnnTest2/clusteringXXX.py:24: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W0829 10:26:11.528053 140393929348928 deprecation_wrapper.py:119] From /media/daniel/D/code/cnnTest2/clusteringXXX.py:38: The name tf.segment_sum is deprecated. Please use tf.math.segment_sum instead.\n",
      "\n",
      "W0829 10:26:11.530549 140393929348928 deprecation.py:506] From /media/daniel/D/code/cnnTest2/clusteringXXX.py:39: calling norm (from tensorflow.python.ops.linalg_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "W0829 10:26:11.536710 140393929348928 deprecation_wrapper.py:119] From /media/daniel/D/code/cnnTest2/clusteringXXX.py:49: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "W0829 10:26:11.574047 140393929348928 deprecation.py:323] From /home/daniel/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0829 10:26:11.631249 140393929348928 deprecation_wrapper.py:119] From /media/daniel/D/code/cnnTest2/clusteringXXX.py:58: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0829 10:26:11.766143 140393929348928 deprecation_wrapper.py:119] From /media/daniel/D/code/cnnTest2/clusteringXXX.py:59: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172.49942\n",
      "0.05228438\n",
      "0.07820205\n",
      "0.09317509\n",
      "0.103439696\n",
      "0.19104102\n",
      "0.19554973\n",
      "0.18891373\n",
      "0.1390349\n",
      "0.14985354\n",
      "0.1899344\n",
      "0.30339804\n",
      "0.30216148\n",
      "0.25925338\n",
      "0.43205383\n",
      "0.24620059\n",
      "0.34443945\n",
      "0.20729384\n",
      "0.04919281\n",
      "0.07043117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0829 10:27:41.017272 140393929348928 deprecation_wrapper.py:119] From /media/daniel/D/code/cnnTest2/clusteringXXX.py:51: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023612397\n",
      "0.7342072\n",
      "0.6283493\n",
      "0.28975528\n",
      "0.1365474\n",
      "0.55786407\n",
      "0.061397363\n",
      "0.35283086\n",
      "0.51903296\n",
      "0.19624573\n",
      "0.0497529\n",
      "0.40818334\n",
      "0.33392963\n",
      "0.043432754\n",
      "0.35868123\n",
      "0.041473266\n",
      "0.36334875\n",
      "0.046848353\n",
      "0.055120353\n",
      "0.18601958\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from clusteringXXX import cluster_data\n",
    "from clusteringXXX import cluster_project\n",
    "scaler = StandardScaler(copy=True, with_mean=True, with_std=False).fit(feat)\n",
    "newFeat = scaler.transform(feat, copy=True)                           \n",
    "newFeat = newFeat/np.linalg.norm(newFeat, keepdims =True, axis =1)\n",
    "\n",
    "\n",
    "w, sig, labelK = cluster_data(newFeat, numClass=10, add_dim_per_class=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "0.8143076923076923\n",
      "0.9490778630938214\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from cluster_eval import eval_labels\n",
    "\n",
    "hyper_label, hyper_feat = cluster_project(newFeat, w, sig, 10)\n",
    "\n",
    "a, b = eval_labels(hyper_label, gt, np.max(hyper_feat, axis =1))\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster_eval import eval_kmeans\n",
    "\n",
    "eval_kmeans(allFeat, 10, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "converge_labels2() missing 1 required positional argument: 'estimated_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-983c7f571fb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mnew_gt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverge_labels2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyper_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mpurity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyper_label\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnew_gt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: converge_labels2() missing 1 required positional argument: 'estimated_classes'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from cluster_eval import converge_labels2\n",
    "from cluster_eval import eval_labels\n",
    "\n",
    "hyper_label, hyper_feat = cluster_project(newFeat, w, sig, 10)\n",
    "\n",
    "\n",
    "new_gt = converge_labels2(hyper_label, gt.astype(int), n)\n",
    "\n",
    "purity = np.sum((hyper_label == new_gt))/gt.size\n",
    "mean_average_precision = average_precision_score(\n",
    "    new_labels == gt, np.max(hyper_feat, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9996729928155637"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "\n",
    "from cluster_eval import converge_labels\n",
    "from cluster_eval import cluster_acc\n",
    "\n",
    "new_labels = converge_labels(hyper_label, gt.astype(int))\n",
    "\n",
    "purity = np.sum((new_labels == gt))/gt.size\n",
    "average_precision_score(new_labels == gt, np.max(hyper_feat, axis =1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[553 791]\n",
      " [747 509]]\n",
      "0.5915384615384616\n",
      "(2600, 2)\n"
     ]
    }
   ],
   "source": [
    "def score_dimensions(d, thres =0.1):\n",
    "    labels = np.argmax(d, axis =1)\n",
    "    scores = np.zeros(num_clus)\n",
    "    for i in range(labels.size):\n",
    "        lab = labels[i]\n",
    "        scores[lab] = scores[lab] + d[i,lab]\n",
    "    scores = scores/max(scores)\n",
    "    mask = scores >thres\n",
    "    \n",
    "    d_c = d[:,mask]\n",
    "    label_c = np.argmax(d_c, axis =1)\n",
    "    return d_c, label_c\n",
    "        \n",
    "d_c, label_c = score_dimensions(dd)\n",
    "\n",
    "score, aff, _, _ = cluster_acc(label_c, gt.astype(int))\n",
    "print(aff)\n",
    "print(cluster_acc2(aff, d_c.shape[1]))\n",
    "print(d_c.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
