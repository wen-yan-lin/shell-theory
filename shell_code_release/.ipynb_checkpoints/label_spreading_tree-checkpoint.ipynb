{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# allFeat = np.load('/tf/notebooks/clusteringTree/stl_feat/feats_2048.npy')\n",
    "# gtAll = np.load('/tf/notebooks/clusteringTree/stl_feat/labels_2048.npy')-1\n",
    "\n",
    "allFeat = np.load('/tf/notebooks/STL-10/danielFeat.npy')\n",
    "gtAll = np.load('/tf/notebooks/STL-10/danielGt.npy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from clusteringX2 import normIt\n",
    "from clusteringX2 import clusterModel\n",
    "from clusteringX2 import train_step3\n",
    "\n",
    "\n",
    "def clustering(feat, numClass=60, thres=0.1):\n",
    "    feat, m = normIt(feat)\n",
    "    \n",
    "    # initilization\n",
    "    kmeans = KMeans(n_clusters=numClass, random_state=0).fit(feat)\n",
    "    labelK = kmeans.predict(feat)\n",
    "    means =np.zeros([numClass, feat.shape[1]])\n",
    "\n",
    "    for i in range(numClass):\n",
    "        mask = labelK == i\n",
    "        means[i,:] = np.mean(feat[mask,:], axis =0)\n",
    "\n",
    "    sigInit = np.zeros([numClass])\n",
    "    for i in range(numClass):\n",
    "        mask = np.logical_not(labelK == i)\n",
    "        sigInit[i] = np.mean(np.matmul(feat[mask,:], np.transpose(means[i,:])))\n",
    "    model = clusterModel(means.transpose(), sigInit)\n",
    "    train_step3(model, feat.astype(\"float32\"), iter =100)\n",
    "    \n",
    "    \n",
    "    \n",
    "    c = model(feat.astype(\"float32\"))\n",
    "    s = np.max(c, axis=1)\n",
    "    t = np.mean(s)\n",
    "    mask = s > t*0.5\n",
    "    train_step3(model, feat[mask,:].astype(\"float32\"), iter =100)\n",
    "    \n",
    "    \n",
    "    c = model(feat.astype(\"float32\"))\n",
    "    s = np.max(c, axis=1)\n",
    "    t = np.mean(s)\n",
    "    mask = s > t*0.5\n",
    "    train_step3(model, feat[mask,:].astype(\"float32\"), iter =100)\n",
    "\n",
    " \n",
    "    c = model(feat.astype(\"float32\"))\n",
    "    labels = np.argmax(c, axis =1)\n",
    "    mask_ = np.zeros(numClass, dtype=bool)\n",
    "        \n",
    "    for i in range(numClass):\n",
    "        if np.sum(labels==i)>0:\n",
    "            mask_[i] = 1\n",
    "    w = model.w.numpy()\n",
    "    sig = model.sig.numpy()\n",
    "    w = w[:,mask_]\n",
    "    sig = sig[mask_]\n",
    "            \n",
    "\n",
    "    return w, sig, m, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, mask=None, w=None, sig=None, m=None):\n",
    "        self.mask = mask\n",
    "        self.w = w\n",
    "        self.sig = sig\n",
    "        self.m = m\n",
    "            \n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.nodeList = []\n",
    "        \n",
    "class Tree:\n",
    "    def __init__(self):\n",
    "        self.numLayers = 0\n",
    "        self.listOfLayers = []\n",
    "        self.numNodes = 0\n",
    "        self.listOfNodes = []\n",
    "        \n",
    "    def addNode(self, c):\n",
    "        self.listOfNodes.append(c)\n",
    "        self.numNodes = self.numNodes + 1\n",
    "        \n",
    "    def addLayer(self, c):\n",
    "        self.listOfLayers.append(c)\n",
    "        self.numLayers = self.numLayers + 1\n",
    "\n",
    "    \n",
    "def startTree(allFeat, numClass=2):\n",
    "    tree = Tree()    \n",
    "    w, sig, m, inMask = clustering(allFeat, numClass)\n",
    "    mask = np.ones(allFeat.shape[0], dtype=bool)\n",
    "    #mask[np.logical_not(inMask)] = 0\n",
    "    n = Node(mask, w, sig, m)\n",
    "\n",
    "    tree.addNode(n)\n",
    "    newLayer = Layer()\n",
    "    newLayer.nodeList.append(tree.numNodes-1)\n",
    "    tree.addLayer(newLayer)\n",
    "    \n",
    "    return tree\n",
    "\n",
    "    \n",
    "#mask = np.logical_or(gtAll==3,  gtAll==4)\n",
    "mask = gtAll<8\n",
    "\n",
    "feat = allFeat[mask,:]\n",
    "gt = gtAll[mask]    \n",
    "gtCoarseAll = np.logical_or.reduce( [gtAll == 0, gtAll == 2, gtAll == 8, gtAll == 9]).astype(int)   \n",
    "gtCoarse = gtCoarseAll[mask]\n",
    "\n",
    "tree = startTree(feat[::2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataProjection(feat, node):\n",
    "    x, _ = normIt(feat, node.m) \n",
    "    return np.matmul(x, node.w) - node.sig\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def extendTree(tree, allFeat, numClass=2, clusteringThreshold=100):\n",
    "    newLayer = Layer()\n",
    "    lastLayer = tree.listOfLayers[-1]\n",
    "    for nodeIndex in lastLayer.nodeList:\n",
    "        node = tree.listOfNodes[nodeIndex]\n",
    "        curFeat = allFeat[node.mask,:]\n",
    "        score = dataProjection(curFeat, node)\n",
    "        #plt.scatter(score[:,0], score[:,1], alpha=0.1)\n",
    "        \n",
    "        clusterInd = np.argmax(score, axis =1)\n",
    "        curInd = np.where(node.mask)[0]\n",
    "\n",
    "        for i in range(score.shape[1]):\n",
    "            subMask = clusterInd == i\n",
    "            if sum(subMask) > clusteringThreshold:\n",
    "                \n",
    "                newMask =  np.zeros(allFeat.shape[0], dtype=bool)\n",
    "                newMask[curInd[subMask]] = 1\n",
    "                \n",
    "\n",
    "                w, sig, m, inMask = clustering(allFeat[newMask,:], numClass)\n",
    "                #subInd = np.where(newMask)[0]\n",
    "                #newMask[subInd[np.logical_not(inMask)]] = 0\n",
    "\n",
    "                if w.shape[1]>=2:\n",
    "                    n = Node(newMask, w, sig, m)\n",
    "                    tree.addNode(n)\n",
    "                    newLayer.nodeList.append(tree.numNodes-1)\n",
    "    tree.addLayer(newLayer)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "for k in range(100):\n",
    "    extendTree(tree, feat[::2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projectTree2(feat, tree, maxClus=2, unit_vector=True, scale2data=True, verboise=True):\n",
    "    \n",
    "    projectionSpace = np.zeros([feat.shape[0], tree.numNodes*maxClus])\n",
    "    \n",
    "    cur = 0\n",
    "    \n",
    "    for i, n in enumerate(tree.listOfNodes):\n",
    "        \n",
    "        f = dataProjection(feat, n)\n",
    "        numDim = f.shape[1]\n",
    "        if unit_vector:\n",
    "            f_ = np.concatenate([f, 10*np.ones([f.shape[0],1])], axis =1)\n",
    "            f_ = f_/np.linalg.norm(f_, axis =1, keepdims=True)\n",
    "            f = f_[:,:numDim]\n",
    "        if scale2data:\n",
    "            f = f*np.sum(n.mask)/n.mask.size \n",
    "        projectionSpace[:, cur:cur+numDim] = f\n",
    "        cur = cur+numDim\n",
    "        \n",
    "        if verboise == True:\n",
    "            print(i, n)\n",
    "    projectionSpace = projectionSpace[:,:cur]\n",
    "    return projectionSpace\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Feature\n",
      "0 <__main__.Node object at 0x7f385d9ad7f0>\n",
      "1 <__main__.Node object at 0x7f37b3438630>\n",
      "2 <__main__.Node object at 0x7f37c0606c18>\n",
      "3 <__main__.Node object at 0x7f37a82c2c18>\n",
      "4 <__main__.Node object at 0x7f37b9e3de48>\n",
      "5 <__main__.Node object at 0x7f37a82dcef0>\n",
      "6 <__main__.Node object at 0x7f37b9d408d0>\n",
      "7 <__main__.Node object at 0x7f37b9d16eb8>\n",
      "8 <__main__.Node object at 0x7f37b9b9c0b8>\n",
      "9 <__main__.Node object at 0x7f37b9b66da0>\n",
      "10 <__main__.Node object at 0x7f37b99c9d30>\n",
      "11 <__main__.Node object at 0x7f37b99e3ef0>\n",
      "12 <__main__.Node object at 0x7f37b9bfbb00>\n",
      "13 <__main__.Node object at 0x7f37b99059e8>\n",
      "14 <__main__.Node object at 0x7f37b982ef28>\n",
      "15 <__main__.Node object at 0x7f37b9c69eb8>\n",
      "16 <__main__.Node object at 0x7f37b9761cc0>\n",
      "17 <__main__.Node object at 0x7f37b96c8e80>\n",
      "18 <__main__.Node object at 0x7f37b96f0c50>\n",
      "19 <__main__.Node object at 0x7f37b9681b00>\n",
      "20 <__main__.Node object at 0x7f37b95fd390>\n",
      "21 <__main__.Node object at 0x7f37b95f51d0>\n",
      "22 <__main__.Node object at 0x7f37b95c6e48>\n",
      "23 <__main__.Node object at 0x7f37b96e85c0>\n",
      "24 <__main__.Node object at 0x7f37b95b3cc0>\n",
      "25 <__main__.Node object at 0x7f37b948ea20>\n",
      "26 <__main__.Node object at 0x7f37b94036d8>\n",
      "27 <__main__.Node object at 0x7f37b93c2048>\n",
      "28 <__main__.Node object at 0x7f37b93f5e48>\n",
      "29 <__main__.Node object at 0x7f37b92c0cc0>\n",
      "30 <__main__.Node object at 0x7f37b9246b00>\n",
      "31 <__main__.Node object at 0x7f37b9349e10>\n",
      "32 <__main__.Node object at 0x7f37b91cd160>\n",
      "33 <__main__.Node object at 0x7f37b915bbe0>\n",
      "34 <__main__.Node object at 0x7f37b9146550>\n",
      "35 <__main__.Node object at 0x7f37b91b2cf8>\n",
      "36 <__main__.Node object at 0x7f37b906ffd0>\n",
      "37 <__main__.Node object at 0x7f37b9045dd8>\n",
      "38 <__main__.Node object at 0x7f37b8feec18>\n",
      "39 <__main__.Node object at 0x7f37b8f618d0>\n",
      "40 <__main__.Node object at 0x7f37b8fa40f0>\n",
      "41 <__main__.Node object at 0x7f37b91bfa58>\n",
      "42 <__main__.Node object at 0x7f37b8e78668>\n",
      "43 <__main__.Node object at 0x7f37b8de86d8>\n",
      "44 <__main__.Node object at 0x7f37b8e20f60>\n",
      "45 <__main__.Node object at 0x7f37b90c1a20>\n",
      "46 <__main__.Node object at 0x7f37b8d1fb70>\n",
      "47 <__main__.Node object at 0x7f37b8cb3f28>\n",
      "48 <__main__.Node object at 0x7f37b8c33fd0>\n",
      "49 <__main__.Node object at 0x7f37b8b99eb8>\n",
      "50 <__main__.Node object at 0x7f37b8b53f28>\n",
      "51 <__main__.Node object at 0x7f37b8b4d668>\n",
      "52 <__main__.Node object at 0x7f37b8b11780>\n",
      "53 <__main__.Node object at 0x7f37b8acc080>\n",
      "54 <__main__.Node object at 0x7f37b8b40eb8>\n",
      "55 <__main__.Node object at 0x7f37b8a4e438>\n",
      "56 <__main__.Node object at 0x7f37b8a26d68>\n",
      "57 <__main__.Node object at 0x7f37b89b1710>\n",
      "58 <__main__.Node object at 0x7f37b89b8978>\n",
      "59 <__main__.Node object at 0x7f37b88dd4e0>\n",
      "60 <__main__.Node object at 0x7f37b8895cf8>\n",
      "61 <__main__.Node object at 0x7f37b887bf60>\n",
      "62 <__main__.Node object at 0x7f37b885a208>\n",
      "63 <__main__.Node object at 0x7f37b8770518>\n",
      "64 <__main__.Node object at 0x7f37b8854668>\n",
      "65 <__main__.Node object at 0x7f37b8752e10>\n",
      "66 <__main__.Node object at 0x7f37b87654a8>\n",
      "67 <__main__.Node object at 0x7f37b869ce80>\n",
      "68 <__main__.Node object at 0x7f37b875eb70>\n",
      "69 <__main__.Node object at 0x7f37b8680128>\n",
      "70 <__main__.Node object at 0x7f37b85eadd8>\n",
      "71 <__main__.Node object at 0x7f37b8642198>\n",
      "72 <__main__.Node object at 0x7f37b8532fd0>\n",
      "73 <__main__.Node object at 0x7f37b8529208>\n",
      "74 <__main__.Node object at 0x7f37b84aec50>\n",
      "75 <__main__.Node object at 0x7f37b858f208>\n",
      "76 <__main__.Node object at 0x7f37b8441ac8>\n",
      "77 <__main__.Node object at 0x7f37b85d0e10>\n",
      "78 <__main__.Node object at 0x7f37b83d0e80>\n",
      "79 <__main__.Node object at 0x7f37b8421908>\n",
      "80 <__main__.Node object at 0x7f37b83b4d30>\n",
      "81 <__main__.Node object at 0x7f37b832c5c0>\n",
      "82 <__main__.Node object at 0x7f37b861bb70>\n",
      "83 <__main__.Node object at 0x7f37b8268240>\n",
      "84 <__main__.Node object at 0x7f37b8299710>\n",
      "85 <__main__.Node object at 0x7f37b81e2c50>\n",
      "86 <__main__.Node object at 0x7f37b8204dd8>\n",
      "87 <__main__.Node object at 0x7f37b8177ac8>\n",
      "88 <__main__.Node object at 0x7f37b83848d0>\n",
      "89 <__main__.Node object at 0x7f37b8246278>\n",
      "90 <__main__.Node object at 0x7f37b80c8ba8>\n",
      "num dimenions: 182\n"
     ]
    }
   ],
   "source": [
    "print('Create Feature')\n",
    "\n",
    "projectionSpace = projectTree2(feat, tree, unit_vector=True, scale2data=True)\n",
    "pp = projectionSpace.copy()\n",
    "pp_s, _ = normIt(pp)\n",
    "\n",
    "print('num dimenions:', pp_s.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create UMAP Feature\n",
      "Processing /root/.cache/pip/wheels/d0/f8/d5/8e3af3ee957feb9b403a060ebe72f7561887fef9dea658326e/umap_learn-0.3.10-cp36-none-any.whl\n",
      "Requirement already satisfied: scikit-learn>=0.16 in /usr/local/lib/python3.6/dist-packages (from umap.learn) (0.22.1)\n",
      "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.6/dist-packages (from umap.learn) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.6/dist-packages (from umap.learn) (1.18.1)\n",
      "Requirement already satisfied: numba>=0.37 in /usr/local/lib/python3.6/dist-packages (from umap.learn) (0.48.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.16->umap.learn) (0.14.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba>=0.37->umap.learn) (44.0.0)\n",
      "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.37->umap.learn) (0.31.0)\n",
      "Installing collected packages: umap.learn\n",
      "Successfully installed umap.learn\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numba/typed_passes.py:293: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../../usr/local/lib/python3.6/dist-packages/umap/rp_tree.py\", line 135:\n",
      "@numba.njit(fastmath=True, nogil=True, parallel=True)\n",
      "def euclidean_random_projection_split(data, indices, rng_state):\n",
      "^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/usr/local/lib/python3.6/dist-packages/umap/nndescent.py:92: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../../usr/local/lib/python3.6/dist-packages/umap/utils.py\", line 409:\n",
      "@numba.njit(parallel=True)\n",
      "def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):\n",
      "^\n",
      "\n",
      "  current_graph, n_vertices, n_neighbors, max_candidates, rng_state\n",
      "/usr/local/lib/python3.6/dist-packages/numba/typed_passes.py:293: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../../usr/local/lib/python3.6/dist-packages/umap/nndescent.py\", line 47:\n",
      "    @numba.njit(parallel=True)\n",
      "    def nn_descent(\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    }
   ],
   "source": [
    "print('Create UMAP Feature')\n",
    "\n",
    "! pip3 install umap.learn\n",
    "import umap\n",
    "\n",
    "\n",
    "\n",
    "f_norm, _ = normIt(feat)\n",
    "embedding = umap.UMAP()\n",
    "fUmap = embedding.fit_transform(f_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Spreading evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "# setup\n",
    "lModel = LabelSpreading(gamma=100)\n",
    "gt_use = gt.copy()\n",
    "#gt_use = gtCoarse.copy()\n",
    "#########\n",
    "\n",
    "\n",
    "samplePerClass = 1\n",
    "numClass = int(max(gt_use)+1)\n",
    "ind = []\n",
    "for i in range(numClass):\n",
    "    index = list(np.where(gt_use==i)[0])\n",
    "    if len(index) == 0:\n",
    "        continue\n",
    "    index = random.sample(index, samplePerClass);\n",
    "    ind = ind + (index)\n",
    "#ind = random.sample(range(pp_s.shape[0]), numClass*samplePerClass);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ours\n",
      "Precision: 0.7791538461538462\n",
      "map: 0.9224741392375095\n"
     ]
    }
   ],
   "source": [
    "print('Ours')\n",
    "\n",
    "\n",
    "lModel.fit(pp_s[ind], gt_use[ind])\n",
    "label = lModel.predict(pp_s)\n",
    "score = np.max(lModel.predict_proba(pp_s), axis=1)\n",
    "\n",
    "\n",
    "print('Precision:', np.sum(label==gt_use)/label.size)\n",
    "print('map:', average_precision_score(label==gt_use, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA\n",
      "Precision: 0.6347692307692308\n",
      "map: 0.7410599740274268\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print('PCA')\n",
    "\n",
    "f_norm, _ = normIt(feat)\n",
    "\n",
    "pca = PCA(n_components=pp.shape[1])\n",
    "f_pca = pca.fit_transform(f_norm)\n",
    "\n",
    "lModel.fit(f_pca[ind], gt_use[ind])\n",
    "label = lModel.predict(f_pca)\n",
    "score = np.max(lModel.predict_proba(f_pca), axis=1)\n",
    "\n",
    "print('Precision:', np.sum(label==gt_use)/label.size)\n",
    "print('map:', average_precision_score(label==gt_use, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/semi_supervised/_label_propagation.py:201: RuntimeWarning: invalid value encountered in true_divide\n",
      "  probabilities /= normalizer\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/semi_supervised/_label_propagation.py:201: RuntimeWarning: invalid value encountered in true_divide\n",
      "  probabilities /= normalizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9810769230769231\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1bba2e9958f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Precision:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mgt_use\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'map:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_precision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mgt_use\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36maverage_precision_score\u001b[0;34m(y_true, y_score, average, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    213\u001b[0m                                 pos_label=pos_label)\n\u001b[1;32m    214\u001b[0m     return _average_binary_score(average_precision, y_true, y_score,\n\u001b[0;32m--> 215\u001b[0;31m                                  average, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_uninterpolated_average_precision\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    194\u001b[0m             y_true, y_score, pos_label=1, sample_weight=None):\n\u001b[1;32m    195\u001b[0m         precision, recall, _ = precision_recall_curve(\n\u001b[0;32m--> 196\u001b[0;31m             y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;31m# Return the step function integral\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# The following works because the last entry of precision is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mprecision_recall_curve\u001b[0;34m(y_true, probas_pred, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    671\u001b[0m     fps, tps, thresholds = _binary_clf_curve(y_true, probas_pred,\n\u001b[1;32m    672\u001b[0m                                              \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m                                              sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtps\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtps\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0massert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m     \u001b[0massert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36massert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mallow_nan\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \"\"\"\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     (type_err,\n\u001b[0;32m---> 60\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "print('Naive')\n",
    "\n",
    "\n",
    "\n",
    "lModel.fit(f_norm[ind], gt_use[ind])\n",
    "label = lModel.predict(f_norm)\n",
    "score = np.max(lModel.predict_proba(f_norm), axis=1)\n",
    "\n",
    "\n",
    "print('Precision:', np.sum(label==gt_use)/label.size)\n",
    "print('map:', average_precision_score(label==gt_use, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP\n",
      "Precision: 0.9678461538461538\n",
      "map: 0.9949221686409679\n"
     ]
    }
   ],
   "source": [
    "print('UMAP')\n",
    "\n",
    "lModel.fit(fUmap[ind], gt_use[ind])\n",
    "label = lModel.predict(fUmap)\n",
    "score = np.max(lModel.predict_proba(fUmap), axis=1)\n",
    "\n",
    "print('Precision:', np.sum(label==gt_use)/label.size)\n",
    "print('map:', average_precision_score(label==gt_use, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Feature Goodness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def test_feat(feat, labels, nBest= 1000):\n",
    "    num_feat = feat.shape[0]\n",
    "    d = pairwise_distances(feat)\n",
    "    ind = np.argsort(d, axis =1)\n",
    "    \n",
    "    err = np.zeros(num_feat)\n",
    "    \n",
    "    for i in range(num_feat):\n",
    "        l = labels[ind[i,:nBest+1]]      \n",
    "        err[i] = nBest - (sum((l-l[0]) == 0))+1\n",
    "                          \n",
    "    return err\n",
    "\n",
    "\n",
    "\n",
    "nBest = 250\n",
    "\n",
    "err = test_feat(pp_s[1::2], gt[1::2], nBest=nBest)\n",
    "print('average error of  %f from %d nearest neighbors'  %(np.mean(err), nBest))\n",
    "\n",
    "print('num dimenions:', pp.shape[1])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
