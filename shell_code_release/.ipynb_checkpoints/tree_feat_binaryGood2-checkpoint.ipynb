{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "allFeat = np.load('/tf/notebooks/clusteringTree/stl_feat/feats_2048.npy')\n",
    "gtAll = np.load('/tf/notebooks/clusteringTree/stl_feat/labels_2048.npy')-1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from clusteringX2 import normIt\n",
    "from clusteringX2 import clusterModel\n",
    "from clusteringX2 import train_step3\n",
    "\n",
    "\n",
    "def clustering(feat, numClass=60, thres=0.1):\n",
    "    feat, m = normIt(feat)\n",
    "    \n",
    "    # initilization\n",
    "    kmeans = KMeans(n_clusters=numClass, random_state=0).fit(feat)\n",
    "    labelK = kmeans.predict(feat)\n",
    "    means =np.zeros([numClass, feat.shape[1]])\n",
    "\n",
    "    for i in range(numClass):\n",
    "        mask = labelK == i\n",
    "        means[i,:] = np.mean(feat[mask,:], axis =0)\n",
    "\n",
    "    sigInit = np.zeros([numClass])\n",
    "    for i in range(numClass):\n",
    "        mask = np.logical_not(labelK == i)\n",
    "        sigInit[i] = np.mean(np.matmul(feat[mask,:], np.transpose(means[i,:])))\n",
    "    model = clusterModel(means.transpose(), sigInit)\n",
    "    train_step3(model, feat.astype(\"float32\"), iter =100)\n",
    "    \n",
    "    \n",
    "    \n",
    "    c = model(feat.astype(\"float32\"))\n",
    "    s = np.max(c, axis=1)\n",
    "    t = np.mean(s)\n",
    "    mask = s > t*0.5\n",
    "    train_step3(model, feat[mask,:].astype(\"float32\"), iter =100)\n",
    "    \n",
    "    \n",
    "    c = model(feat.astype(\"float32\"))\n",
    "    s = np.max(c, axis=1)\n",
    "    t = np.mean(s)\n",
    "    mask = s > t*0.5\n",
    "    train_step3(model, feat[mask,:].astype(\"float32\"), iter =100)\n",
    "\n",
    " \n",
    "    c = model(feat.astype(\"float32\"))\n",
    "    labels = np.argmax(c, axis =1)\n",
    "    mask_ = np.zeros(numClass, dtype=bool)\n",
    "        \n",
    "    for i in range(numClass):\n",
    "        if np.sum(labels==i)>0:\n",
    "            mask_[i] = 1\n",
    "    w = model.w.numpy()\n",
    "    sig = model.sig.numpy()\n",
    "    w = w[:,mask_]\n",
    "    sig = sig[mask_]\n",
    "            \n",
    "\n",
    "    return w, sig, m, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, mask=None, w=None, sig=None, m=None):\n",
    "        self.mask = mask\n",
    "        self.w = w\n",
    "        self.sig = sig\n",
    "        self.m = m\n",
    "            \n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.nodeList = []\n",
    "        \n",
    "class Tree:\n",
    "    def __init__(self):\n",
    "        self.numLayers = 0\n",
    "        self.listOfLayers = []\n",
    "        self.numNodes = 0\n",
    "        self.listOfNodes = []\n",
    "        \n",
    "    def addNode(self, c):\n",
    "        self.listOfNodes.append(c)\n",
    "        self.numNodes = self.numNodes + 1\n",
    "        \n",
    "    def addLayer(self, c):\n",
    "        self.listOfLayers.append(c)\n",
    "        self.numLayers = self.numLayers + 1\n",
    "\n",
    "    \n",
    "def startTree(allFeat, numClass=2):\n",
    "    tree = Tree()    \n",
    "    w, sig, m, inMask = clustering(allFeat, numClass)\n",
    "    mask = np.ones(allFeat.shape[0], dtype=bool)\n",
    "    #mask[np.logical_not(inMask)] = 0\n",
    "    n = Node(mask, w, sig, m)\n",
    "\n",
    "    tree.addNode(n)\n",
    "    newLayer = Layer()\n",
    "    newLayer.nodeList.append(tree.numNodes-1)\n",
    "    tree.addLayer(newLayer)\n",
    "    \n",
    "    return tree\n",
    "\n",
    "    \n",
    "mask = np.logical_or(gtAll==3,  gtAll==4)\n",
    "#mask = gtAll<10\n",
    "\n",
    "feat = allFeat[mask,:]\n",
    "gt = gtAll[mask]    \n",
    "gtCoarseAll = np.logical_or.reduce( [gtAll == 0, gtAll == 2, gtAll == 8, gtAll == 9]).astype(int)   \n",
    "gtCoarse = gtCoarseAll[mask]\n",
    "\n",
    "tree = startTree(feat[::2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataProjection(feat, node):\n",
    "    x, _ = normIt(feat, node.m) \n",
    "    return np.matmul(x, node.w) - node.sig\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def extendTree(tree, allFeat, numClass=2, clusteringThreshold=100):\n",
    "    newLayer = Layer()\n",
    "    lastLayer = tree.listOfLayers[-1]\n",
    "    for nodeIndex in lastLayer.nodeList:\n",
    "        node = tree.listOfNodes[nodeIndex]\n",
    "        curFeat = allFeat[node.mask,:]\n",
    "        score = dataProjection(curFeat, node)\n",
    "        #plt.scatter(score[:,0], score[:,1], alpha=0.1)\n",
    "        \n",
    "        clusterInd = np.argmax(score, axis =1)\n",
    "        curInd = np.where(node.mask)[0]\n",
    "\n",
    "        for i in range(score.shape[1]):\n",
    "            subMask = clusterInd == i\n",
    "            if sum(subMask) > clusteringThreshold:\n",
    "                \n",
    "                newMask =  np.zeros(allFeat.shape[0], dtype=bool)\n",
    "                newMask[curInd[subMask]] = 1\n",
    "                \n",
    "\n",
    "                w, sig, m, inMask = clustering(allFeat[newMask,:], numClass)\n",
    "                #subInd = np.where(newMask)[0]\n",
    "                #newMask[subInd[np.logical_not(inMask)]] = 0\n",
    "\n",
    "                if w.shape[1]>=2:\n",
    "                    n = Node(newMask, w, sig, m)\n",
    "                    tree.addNode(n)\n",
    "                    newLayer.nodeList.append(tree.numNodes-1)\n",
    "    tree.addLayer(newLayer)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "for k in range(100):\n",
    "    extendTree(tree, feat[::2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projectTree2(feat, tree, maxClus=2, unit_vector=True, scale2data=True, verboise=True):\n",
    "    \n",
    "    projectionSpace = np.zeros([feat.shape[0], tree.numNodes*maxClus])\n",
    "    \n",
    "    cur = 0\n",
    "    \n",
    "    for i, n in enumerate(tree.listOfNodes):\n",
    "        \n",
    "        f = dataProjection(feat, n)\n",
    "        numDim = f.shape[1]\n",
    "        if unit_vector:\n",
    "            f_ = np.concatenate([f, 10*np.ones([f.shape[0],1])], axis =1)\n",
    "            f_ = f_/np.linalg.norm(f_, axis =1, keepdims=True)\n",
    "            f = f_[:,:numDim]\n",
    "        if scale2data:\n",
    "            f = f*np.sum(n.mask)/n.mask.size \n",
    "        projectionSpace[:, cur:cur+numDim] = f\n",
    "        cur = cur+numDim\n",
    "        \n",
    "        if verboise == True:\n",
    "            print(i, n)\n",
    "    projectionSpace = projectionSpace[:,:cur]\n",
    "    return projectionSpace\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Feature\n",
      "0 <__main__.Node object at 0x7f160002c588>\n",
      "1 <__main__.Node object at 0x7f15373c54a8>\n",
      "2 <__main__.Node object at 0x7f152c1fbd68>\n",
      "3 <__main__.Node object at 0x7f14e4376748>\n",
      "4 <__main__.Node object at 0x7f153dbc0cf8>\n",
      "5 <__main__.Node object at 0x7f153db7e588>\n",
      "6 <__main__.Node object at 0x7f1540070278>\n",
      "7 <__main__.Node object at 0x7f1543b04588>\n",
      "8 <__main__.Node object at 0x7f15200a4d68>\n",
      "9 <__main__.Node object at 0x7f1543673e48>\n",
      "10 <__main__.Node object at 0x7f15439bbf98>\n",
      "11 <__main__.Node object at 0x7f154332f630>\n",
      "12 <__main__.Node object at 0x7f154326b5f8>\n",
      "13 <__main__.Node object at 0x7f1542d00940>\n",
      "14 <__main__.Node object at 0x7f1543041e80>\n",
      "15 <__main__.Node object at 0x7f154290fda0>\n",
      "16 <__main__.Node object at 0x7f15439dc208>\n",
      "17 <__main__.Node object at 0x7f1541d648d0>\n",
      "18 <__main__.Node object at 0x7f1541df7ef0>\n",
      "19 <__main__.Node object at 0x7f1541b495f8>\n",
      "20 <__main__.Node object at 0x7f15420a7c50>\n",
      "21 <__main__.Node object at 0x7f15413737b8>\n",
      "22 <__main__.Node object at 0x7f1541bf9c50>\n",
      "23 <__main__.Node object at 0x7f1542a1df98>\n",
      "24 <__main__.Node object at 0x7f1540f9ea20>\n",
      "25 <__main__.Node object at 0x7f1540f03198>\n",
      "26 <__main__.Node object at 0x7f1541334978>\n",
      "27 <__main__.Node object at 0x7f1541f1f198>\n",
      "28 <__main__.Node object at 0x7f154080ad30>\n",
      "29 <__main__.Node object at 0x7f15407e8320>\n",
      "30 <__main__.Node object at 0x7f1541a1f518>\n",
      "31 <__main__.Node object at 0x7f154122f978>\n",
      "32 <__main__.Node object at 0x7f15405537b8>\n",
      "33 <__main__.Node object at 0x7f15404115f8>\n",
      "34 <__main__.Node object at 0x7f1540911518>\n",
      "35 <__main__.Node object at 0x7f1540baa5f8>\n",
      "36 <__main__.Node object at 0x7f154158fda0>\n",
      "37 <__main__.Node object at 0x7f153fe82a58>\n",
      "38 <__main__.Node object at 0x7f1540c5fc50>\n",
      "39 <__main__.Node object at 0x7f1541fd2198>\n",
      "40 <__main__.Node object at 0x7f153f3dafd0>\n",
      "41 <__main__.Node object at 0x7f153f730240>\n",
      "42 <__main__.Node object at 0x7f153fb54b00>\n",
      "43 <__main__.Node object at 0x7f153f4a5f98>\n",
      "44 <__main__.Node object at 0x7f1540571128>\n",
      "45 <__main__.Node object at 0x7f153fa9b160>\n",
      "46 <__main__.Node object at 0x7f153eeb66a0>\n",
      "47 <__main__.Node object at 0x7f153eef35f8>\n",
      "48 <__main__.Node object at 0x7f153eba9f60>\n",
      "49 <__main__.Node object at 0x7f153ebcce80>\n",
      "50 <__main__.Node object at 0x7f153eb6fa58>\n",
      "51 <__main__.Node object at 0x7f153e976dd8>\n",
      "52 <__main__.Node object at 0x7f153ec99550>\n",
      "53 <__main__.Node object at 0x7f153f0720b8>\n",
      "54 <__main__.Node object at 0x7f153e580828>\n",
      "55 <__main__.Node object at 0x7f153e2a6710>\n",
      "56 <__main__.Node object at 0x7f153e62d710>\n",
      "57 <__main__.Node object at 0x7f153e7b0128>\n",
      "58 <__main__.Node object at 0x7f153db64f98>\n",
      "59 <__main__.Node object at 0x7f153d99a5c0>\n",
      "60 <__main__.Node object at 0x7f153f2161d0>\n",
      "61 <__main__.Node object at 0x7f153d964668>\n",
      "62 <__main__.Node object at 0x7f14e4174898>\n",
      "63 <__main__.Node object at 0x7f153dae1780>\n",
      "64 <__main__.Node object at 0x7f153e348ef0>\n",
      "65 <__main__.Node object at 0x7f153e1f7b00>\n",
      "66 <__main__.Node object at 0x7f153e6c4710>\n",
      "67 <__main__.Node object at 0x7f14bc446898>\n",
      "68 <__main__.Node object at 0x7f14bc26e5f8>\n",
      "69 <__main__.Node object at 0x7f14bc6daef0>\n",
      "70 <__main__.Node object at 0x7f149c78f710>\n",
      "71 <__main__.Node object at 0x7f153e3fe8d0>\n",
      "72 <__main__.Node object at 0x7f149c4ebb70>\n",
      "73 <__main__.Node object at 0x7f14bc129860>\n",
      "74 <__main__.Node object at 0x7f14e42a54a8>\n",
      "75 <__main__.Node object at 0x7f149c256048>\n",
      "76 <__main__.Node object at 0x7f149c1a3630>\n",
      "77 <__main__.Node object at 0x7f149c324828>\n",
      "78 <__main__.Node object at 0x7f14645db710>\n",
      "79 <__main__.Node object at 0x7f1464731898>\n",
      "80 <__main__.Node object at 0x7f146452cb00>\n",
      "81 <__main__.Node object at 0x7f1464347240>\n",
      "82 <__main__.Node object at 0x7f146416d828>\n",
      "83 <__main__.Node object at 0x7f1464422d68>\n",
      "84 <__main__.Node object at 0x7f14640d78d0>\n",
      "85 <__main__.Node object at 0x7f14641aa828>\n",
      "86 <__main__.Node object at 0x7f14584fdb38>\n",
      "87 <__main__.Node object at 0x7f146425c080>\n",
      "88 <__main__.Node object at 0x7f145824cbe0>\n",
      "89 <__main__.Node object at 0x7f1458643470>\n",
      "90 <__main__.Node object at 0x7f14646c9940>\n",
      "91 <__main__.Node object at 0x7f1458376160>\n",
      "92 <__main__.Node object at 0x7f149c6a8eb8>\n",
      "93 <__main__.Node object at 0x7f14642951d0>\n",
      "94 <__main__.Node object at 0x7f14bc2ab828>\n",
      "num dimenions: 190\n"
     ]
    }
   ],
   "source": [
    "print('Create Feature')\n",
    "\n",
    "projectionSpace = projectTree2(feat, tree, unit_vector=True, scale2data=True)\n",
    "pp = projectionSpace.copy()\n",
    "pp_s, _ = normIt(pp)\n",
    "\n",
    "print('num dimenions:', pp_s.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting umap.learn\n",
      "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.6/dist-packages (from umap.learn) (1.17.2)\n",
      "Requirement already satisfied: scikit-learn>=0.16 in /usr/local/lib/python3.6/dist-packages (from umap.learn) (0.22.1)\n",
      "Requirement already satisfied: numba>=0.37 in /usr/local/lib/python3.6/dist-packages (from umap.learn) (0.48.0)\n",
      "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.6/dist-packages (from umap.learn) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.16->umap.learn) (0.14.1)\n",
      "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.37->umap.learn) (0.31.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba>=0.37->umap.learn) (41.2.0)\n",
      "Installing collected packages: umap.learn\n",
      "Successfully installed umap.learn\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "UMAP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numba/typed_passes.py:293: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../../usr/local/lib/python3.6/dist-packages/umap/rp_tree.py\", line 135:\n",
      "@numba.njit(fastmath=True, nogil=True, parallel=True)\n",
      "def euclidean_random_projection_split(data, indices, rng_state):\n",
      "^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/usr/local/lib/python3.6/dist-packages/umap/nndescent.py:92: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../../usr/local/lib/python3.6/dist-packages/umap/utils.py\", line 409:\n",
      "@numba.njit(parallel=True)\n",
      "def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):\n",
      "^\n",
      "\n",
      "  current_graph, n_vertices, n_neighbors, max_candidates, rng_state\n",
      "/usr/local/lib/python3.6/dist-packages/numba/typed_passes.py:293: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../../usr/local/lib/python3.6/dist-packages/umap/nndescent.py\", line 47:\n",
      "    @numba.njit(parallel=True)\n",
      "    def nn_descent(\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    }
   ],
   "source": [
    "print('Create UMAP Feature')\n",
    "\n",
    "\n",
    "! pip3 install umap.learn\n",
    "import umap\n",
    "\n",
    "\n",
    "\n",
    "f_norm, _ = normIt(feat)\n",
    "embedding = umap.UMAP()\n",
    "fUmap = embedding.fit_transform(f_norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Spreading evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "# setup\n",
    "lModel = LabelSpreading()\n",
    "gt_use = gt.copy()\n",
    "#gt_use = gtCoarse.copy()\n",
    "#########\n",
    "\n",
    "\n",
    "samplePerClass = 2\n",
    "numClass = int(max(gt_use)+1)\n",
    "ind = []\n",
    "for i in range(numClass):\n",
    "    index = list(np.where(gt_use==i)[0])\n",
    "    index = random.sample(index, samplePerClass);\n",
    "    ind = ind + (index)\n",
    "#ind = random.sample(range(pp_s.shape[0]), numClass*samplePerClass);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ours\n",
      "Precision: 0.8249230769230769\n",
      "map: 0.9410530992860375\n"
     ]
    }
   ],
   "source": [
    "print('Ours')\n",
    "\n",
    "\n",
    "lModel.fit(pp_s[ind], gt_use[ind])\n",
    "label = lModel.predict(pp_s)\n",
    "score = np.max(lModel.predict_proba(pp_s), axis=1)\n",
    "\n",
    "\n",
    "print('Precision:', np.sum(label==gt_use)/label.size)\n",
    "print('map:', average_precision_score(label==gt_use, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA\n",
      "Precision: 0.821076923076923\n",
      "map: 0.9619626314074534\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print('PCA')\n",
    "\n",
    "f_norm, _ = normIt(feat)\n",
    "\n",
    "pca = PCA(n_components=pp.shape[1])\n",
    "f_pca = pca.fit_transform(f_norm)\n",
    "\n",
    "lModel.fit(f_pca[ind], gt_use[ind])\n",
    "label = lModel.predict(f_pca)\n",
    "score = np.max(lModel.predict_proba(f_pca), axis=1)\n",
    "\n",
    "print('Precision:', np.sum(label==gt_use)/label.size)\n",
    "print('map:', average_precision_score(label==gt_use, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive\n",
      "Precision: 0.8153846153846154\n",
      "map: 0.9585109167873119\n"
     ]
    }
   ],
   "source": [
    "print('Naive')\n",
    "\n",
    "\n",
    "\n",
    "lModel.fit(f_norm[ind], gt_use[ind])\n",
    "label = lModel.predict(f_norm)\n",
    "score = np.max(lModel.predict_proba(f_norm), axis=1)\n",
    "\n",
    "\n",
    "print('Precision:', np.sum(label==gt_use)/label.size)\n",
    "print('map:', average_precision_score(label==gt_use, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP\n",
      "Precision: 0.8533076923076923\n",
      "map: 0.9352644523698006\n"
     ]
    }
   ],
   "source": [
    "print('UMAP')\n",
    "\n",
    "lModel.fit(fUmap[ind], gt_use[ind])\n",
    "label = lModel.predict(fUmap)\n",
    "score = np.max(lModel.predict_proba(fUmap), axis=1)\n",
    "\n",
    "print('Precision:', np.sum(label==gt_use)/label.size)\n",
    "print('map:', average_precision_score(label==gt_use, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Feature Goodness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average error of  64.503538 from 250 nearest neighbors\n",
      "num dimenions: 190\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def test_feat(feat, labels, nBest= 1000):\n",
    "    num_feat = feat.shape[0]\n",
    "    d = pairwise_distances(feat)\n",
    "    ind = np.argsort(d, axis =1)\n",
    "    \n",
    "    err = np.zeros(num_feat)\n",
    "    \n",
    "    for i in range(num_feat):\n",
    "        l = labels[ind[i,:nBest+1]]      \n",
    "        err[i] = nBest - (sum((l-l[0]) == 0))+1\n",
    "                          \n",
    "    return err\n",
    "\n",
    "\n",
    "\n",
    "nBest = 250\n",
    "\n",
    "err = test_feat(pp_s[1::2], gt[1::2], nBest=nBest)\n",
    "print('average error of  %f from %d nearest neighbors'  %(np.mean(err), nBest))\n",
    "\n",
    "print('num dimenions:', pp.shape[1])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
