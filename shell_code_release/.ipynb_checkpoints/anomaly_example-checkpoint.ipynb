{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "import dirUtil \n",
    "from highDimLearning import vgg2feat\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os\n",
    "import random \n",
    "    \n",
    "\n",
    "def feat2labels(feat):\n",
    "    numClass = len(feat)\n",
    "    numDim = feat[0].shape[1]\n",
    "    numPts = 0\n",
    "    for i in range(numClass):\n",
    "        numPts = numPts + feat[i].shape[0]\n",
    "\n",
    "    allFeat = np.zeros([numPts, numDim])\n",
    "    allLabels = np.zeros(numPts, dtype = int)\n",
    "    cur = 0\n",
    "    for i in range(numClass):\n",
    "        allFeat[cur:cur+feat[i].shape[0],:] = feat[i]\n",
    "        allLabels[cur:cur+feat[i].shape[0]] = i\n",
    "        cur = cur + feat[i].shape[0]\n",
    "    return allFeat, allLabels\n",
    "        \n",
    "    \n",
    "def readNetVlad(folder, numDim =4096):\n",
    "    folderName = os.path.basename(os.path.normpath(folder))\n",
    "    print(folderName)\n",
    "    file = folder +'/vd16_pitts30k_conv5_3_vlad_preL2_intra_white_' +folderName + '_db.bin'    \n",
    "    data = np.fromfile(file, '<f4')\n",
    "    numPts = int(len(data)/numDim)\n",
    "    return np.reshape(data, [numPts, numDim])\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def cluster_acc(Y_pred, Y):\n",
    "    assert Y_pred.size == Y.size\n",
    "    D = max(Y_pred.max(), Y.max())+1\n",
    "    w = np.zeros((D,D), dtype=np.int64)\n",
    "    for i in range(Y_pred.size):\n",
    "        w[Y_pred[i], Y[i]] += 1\n",
    "    \n",
    "    row_ind, col_ind = linear_sum_assignment(w.max() - w)\n",
    "    \n",
    "    print(w[row_ind,col_ind])\n",
    "    return w[row_ind,col_ind].sum()/Y_pred.size, w\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "feat = np.load('/media/daniel/D/Data/STL-10/feats_2048.npy')\n",
    "gt = np.load('/media/daniel/D/Data/STL-10/labels_2048.npy')-1\n",
    "\n",
    "mask = gt>-1#np.logical_or(gt==3, gt==4)\n",
    "feat = feat[mask,:]\n",
    "gt = gt[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "K.clear_session()\n",
    "folder = '/media/daniel/D/code/keras-tutorial/animals/dogs/'\n",
    "feats = dirUtil.dir2vgg16(folder)\n",
    "feat1 = vgg2feat(feats)\n",
    "\n",
    "\n",
    "folder2 = '/media/daniel/D/code/keras-tutorial/animals/cats/'\n",
    "tf.reset_default_graph()\n",
    "K.clear_session()\n",
    "feats = dirUtil.dir2vgg16(folder2)\n",
    "feat2 = vgg2feat(feats)\n",
    "\n",
    "folder3 = '/media/daniel/D/code/keras-tutorial/animals/panda/'\n",
    "tf.reset_default_graph()\n",
    "K.clear_session()\n",
    "feats = dirUtil.dir2vgg16(folder3)\n",
    "feat3 = vgg2feat(feats)\n",
    "\n",
    "feat, gt = feat2labels([feat1, feat2, feat3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alley\n",
      "Flickr11K\n"
     ]
    }
   ],
   "source": [
    "#feat1 = readNetVlad('/media/daniel/D/Data/basicMITData/abbey')\n",
    "    \n",
    "#feat2 = readNetVlad('/media/daniel/D/Data/basicMITData/airport_terminal')\n",
    "\n",
    "feat3 = readNetVlad('/media/daniel/D/Data/basicMITData/alley')\n",
    "\n",
    "# feat4 = readNetVlad('/media/daniel/D/Data/basicMITData/amphitheater')\n",
    "\n",
    "feat_test = readNetVlad('/media/daniel/D/Data/Flickr11K')\n",
    "feat, gt = feat2labels([feat3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Training process completed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<hyperFitProjection.HyperFitProjector at 0x7f64d403e668>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "\n",
    "\n",
    "from hyperFitProjection import HyperFitProjector\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "proj = HyperFitProjector(\"merged\")\n",
    "proj.fit_kmeans(feat, n_cluster=3, n_dim=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from six import string_types\n",
    "\n",
    "class Gaussian_kde_mod(object):\n",
    "    \"\"\"Representation of a kernel-density estimate using Gaussian kernels.\n",
    "\n",
    "    Kernel density estimation is a way to estimate the probability density\n",
    "    function (PDF) of a random variable in a non-parametric way.\n",
    "    `gaussian_kde` works for both uni-variate and multi-variate data.   It\n",
    "    includes automatic bandwidth determination.  The estimation works best for\n",
    "    a unimodal distribution; bimodal or multi-modal distributions tend to be\n",
    "    oversmoothed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : array_like\n",
    "        Datapoints to estimate from. In case of univariate data this is a 1-D\n",
    "        array, otherwise a 2-D array with shape (# of dims, # of data).\n",
    "    bw_method : str, scalar or callable, optional\n",
    "        The method used to calculate the estimator bandwidth.  This can be\n",
    "        'scott', 'silverman', a scalar constant or a callable.  If a scalar,\n",
    "        this will be used directly as `kde.factor`.  If a callable, it should\n",
    "        take a `gaussian_kde` instance as only parameter and return a scalar.\n",
    "        If None (default), 'scott' is used.  See Notes for more details.\n",
    "    weights : array_like, shape (n, ), optional, default: None\n",
    "        An array of weights, of the same shape as `x`.  Each value in `x`\n",
    "        only contributes its associated weight towards the bin count\n",
    "        (instead of 1).\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    dataset : ndarray\n",
    "        The dataset with which `gaussian_kde` was initialized.\n",
    "    d : int\n",
    "        Number of dimensions.\n",
    "    n : int\n",
    "        Number of datapoints.\n",
    "    neff : float\n",
    "        Effective sample size using Kish's approximation.\n",
    "    factor : float\n",
    "        The bandwidth factor, obtained from `kde.covariance_factor`, with which\n",
    "        the covariance matrix is multiplied.\n",
    "    covariance : ndarray\n",
    "        The covariance matrix of `dataset`, scaled by the calculated bandwidth\n",
    "        (`kde.factor`).\n",
    "    inv_cov : ndarray\n",
    "        The inverse of `covariance`.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    kde.evaluate(points) : ndarray\n",
    "        Evaluate the estimated pdf on a provided set of points.\n",
    "    kde(points) : ndarray\n",
    "        Same as kde.evaluate(points)\n",
    "    kde.pdf(points) : ndarray\n",
    "        Alias for ``kde.evaluate(points)``.\n",
    "    kde.set_bandwidth(bw_method='scott') : None\n",
    "        Computes the bandwidth, i.e. the coefficient that multiplies the data\n",
    "        covariance matrix to obtain the kernel covariance matrix.\n",
    "        .. versionadded:: 0.11.0\n",
    "    kde.covariance_factor : float\n",
    "        Computes the coefficient (`kde.factor`) that multiplies the data\n",
    "        covariance matrix to obtain the kernel covariance matrix.\n",
    "        The default is `scotts_factor`.  A subclass can overwrite this method\n",
    "        to provide a different method, or set it through a call to\n",
    "        `kde.set_bandwidth`.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Bandwidth selection strongly influences the estimate obtained from the KDE\n",
    "    (much more so than the actual shape of the kernel).  Bandwidth selection\n",
    "    can be done by a \"rule of thumb\", by cross-validation, by \"plug-in\n",
    "    methods\" or by other means; see [3]_, [4]_ for reviews.  `gaussian_kde`\n",
    "    uses a rule of thumb, the default is Scott's Rule.\n",
    "\n",
    "    Scott's Rule [1]_, implemented as `scotts_factor`, is::\n",
    "\n",
    "        n**(-1./(d+4)),\n",
    "\n",
    "    with ``n`` the number of data points and ``d`` the number of dimensions.\n",
    "    Silverman's Rule [2]_, implemented as `silverman_factor`, is::\n",
    "\n",
    "        (n * (d + 2) / 4.)**(-1. / (d + 4)).\n",
    "\n",
    "    Good general descriptions of kernel density estimation can be found in [1]_\n",
    "    and [2]_, the mathematics for this multi-dimensional implementation can be\n",
    "    found in [1]_.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] D.W. Scott, \"Multivariate Density Estimation: Theory, Practice, and\n",
    "           Visualization\", John Wiley & Sons, New York, Chicester, 1992.\n",
    "    .. [2] B.W. Silverman, \"Density Estimation for Statistics and Data\n",
    "           Analysis\", Vol. 26, Monographs on Statistics and Applied Probability,\n",
    "           Chapman and Hall, London, 1986.\n",
    "    .. [3] B.A. Turlach, \"Bandwidth Selection in Kernel Density Estimation: A\n",
    "           Review\", CORE and Institut de Statistique, Vol. 19, pp. 1-33, 1993.\n",
    "    .. [4] D.M. Bashtannyk and R.J. Hyndman, \"Bandwidth selection for kernel\n",
    "           conditional density estimation\", Computational Statistics & Data\n",
    "           Analysis, Vol. 36, pp. 279-298, 2001.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Generate some random two-dimensional data:\n",
    "\n",
    "    >>> from scipy import stats\n",
    "    >>> def measure(n):\n",
    "    >>>     \"Measurement model, return two coupled measurements.\"\n",
    "    >>>     m1 = np.random.normal(size=n)\n",
    "    >>>     m2 = np.random.normal(scale=0.5, size=n)\n",
    "    >>>     return m1+m2, m1-m2\n",
    "\n",
    "    >>> m1, m2 = measure(2000)\n",
    "    >>> xmin = m1.min()\n",
    "    >>> xmax = m1.max()\n",
    "    >>> ymin = m2.min()\n",
    "    >>> ymax = m2.max()\n",
    "\n",
    "    Perform a kernel density estimate on the data:\n",
    "\n",
    "    >>> X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "    >>> positions = np.vstack([X.ravel(), Y.ravel()])\n",
    "    >>> values = np.vstack([m1, m2])\n",
    "    >>> kernel = stats.gaussian_kde(values)\n",
    "    >>> Z = np.reshape(kernel(positions).T, X.shape)\n",
    "\n",
    "    Plot the results:\n",
    "\n",
    "    >>> import matplotlib.pyplot as plt\n",
    "    >>> fig = plt.figure()\n",
    "    >>> ax = fig.add_subplot(111)\n",
    "    >>> ax.imshow(np.rot90(Z), cmap=plt.cm.gist_earth_r,\n",
    "    ...           extent=[xmin, xmax, ymin, ymax])\n",
    "    >>> ax.plot(m1, m2, 'k.', markersize=2)\n",
    "    >>> ax.set_xlim([xmin, xmax])\n",
    "    >>> ax.set_ylim([ymin, ymax])\n",
    "    >>> plt.show()\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, bw_method=None, weights=None):\n",
    "        self.dataset = np.atleast_2d(dataset)\n",
    "        if not self.dataset.size > 1:\n",
    "            raise ValueError(\"`dataset` input should have multiple elements.\")\n",
    "        self.d, self.n = self.dataset.shape\n",
    "            \n",
    "        if weights is not None:\n",
    "            self.weights = weights / np.sum(weights)\n",
    "        else:\n",
    "            self.weights = np.ones(self.n) / self.n\n",
    "            \n",
    "        # Compute the effective sample size \n",
    "        # http://surveyanalysis.org/wiki/Design_Effects_and_Effective_Sample_Size#Kish.27s_approximate_formula_for_computing_effective_sample_size\n",
    "        self.neff = 1.0 / np.sum(self.weights ** 2)\n",
    "\n",
    "        self.set_bandwidth(bw_method=bw_method)\n",
    "\n",
    "    def evaluate(self, points):\n",
    "        \"\"\"Evaluate the estimated pdf on a set of points.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        points : (# of dimensions, # of points)-array\n",
    "            Alternatively, a (# of dimensions,) vector can be passed in and\n",
    "            treated as a single point.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        values : (# of points,)-array\n",
    "            The values at each point.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError : if the dimensionality of the input points is different than\n",
    "                     the dimensionality of the KDE.\n",
    "\n",
    "        \"\"\"\n",
    "        points = np.atleast_2d(points)\n",
    "\n",
    "        d, m = points.shape\n",
    "        if d != self.d:\n",
    "            if d == 1 and m == self.d:\n",
    "                # points was passed in as a row vector\n",
    "                points = np.reshape(points, (self.d, 1))\n",
    "                m = 1\n",
    "            else:\n",
    "                msg = \"points have dimension %s, dataset has dimension %s\" % (d,\n",
    "                    self.d)\n",
    "                raise ValueError(msg)\n",
    "\n",
    "        # compute the normalised residuals\n",
    "        chi2 = cdist(points.T, self.dataset.T, 'mahalanobis', VI=self.inv_cov) ** 2\n",
    "        # compute the pdf\n",
    "        result = np.sum(np.exp(-.5 * chi2) * self.weights, axis=1) / self._norm_factor\n",
    "\n",
    "        return result\n",
    "\n",
    "    __call__ = evaluate\n",
    "\n",
    "    def scotts_factor(self):\n",
    "        return np.power(self.neff, -1./(self.d+4))\n",
    "\n",
    "    def silverman_factor(self):\n",
    "        return np.power(self.neff*(self.d+2.0)/4.0, -1./(self.d+4))\n",
    "\n",
    "    #  Default method to calculate bandwidth, can be overwritten by subclass\n",
    "    covariance_factor = scotts_factor\n",
    "\n",
    "    def set_bandwidth(self, bw_method=None):\n",
    "        \"\"\"Compute the estimator bandwidth with given method.\n",
    "\n",
    "        The new bandwidth calculated after a call to `set_bandwidth` is used\n",
    "        for subsequent evaluations of the estimated density.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        bw_method : str, scalar or callable, optional\n",
    "            The method used to calculate the estimator bandwidth.  This can be\n",
    "            'scott', 'silverman', a scalar constant or a callable.  If a\n",
    "            scalar, this will be used directly as `kde.factor`.  If a callable,\n",
    "            it should take a `gaussian_kde` instance as only parameter and\n",
    "            return a scalar.  If None (default), nothing happens; the current\n",
    "            `kde.covariance_factor` method is kept.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        .. versionadded:: 0.11\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> x1 = np.array([-7, -5, 1, 4, 5.])\n",
    "        >>> kde = stats.gaussian_kde(x1)\n",
    "        >>> xs = np.linspace(-10, 10, num=50)\n",
    "        >>> y1 = kde(xs)\n",
    "        >>> kde.set_bandwidth(bw_method='silverman')\n",
    "        >>> y2 = kde(xs)\n",
    "        >>> kde.set_bandwidth(bw_method=kde.factor / 3.)\n",
    "        >>> y3 = kde(xs)\n",
    "\n",
    "        >>> fig = plt.figure()\n",
    "        >>> ax = fig.add_subplot(111)\n",
    "        >>> ax.plot(x1, np.ones(x1.shape) / (4. * x1.size), 'bo',\n",
    "        ...         label='Data points (rescaled)')\n",
    "        >>> ax.plot(xs, y1, label='Scott (default)')\n",
    "        >>> ax.plot(xs, y2, label='Silverman')\n",
    "        >>> ax.plot(xs, y3, label='Const (1/3 * Silverman)')\n",
    "        >>> ax.legend()\n",
    "        >>> plt.show()\n",
    "\n",
    "        \"\"\"\n",
    "        if bw_method is None:\n",
    "            pass\n",
    "        elif bw_method == 'scott':\n",
    "            self.covariance_factor = self.scotts_factor\n",
    "        elif bw_method == 'silverman':\n",
    "            self.covariance_factor = self.silverman_factor\n",
    "        elif np.isscalar(bw_method) and not isinstance(bw_method, string_types):\n",
    "            self._bw_method = 'use constant'\n",
    "            self.covariance_factor = lambda: bw_method\n",
    "        elif callable(bw_method):\n",
    "            self._bw_method = bw_method\n",
    "            self.covariance_factor = lambda: self._bw_method(self)\n",
    "        else:\n",
    "            msg = \"`bw_method` should be 'scott', 'silverman', a scalar \" \\\n",
    "                  \"or a callable.\"\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        self._compute_covariance()\n",
    "\n",
    "    def _compute_covariance(self):\n",
    "        \"\"\"Computes the covariance matrix for each Gaussian kernel using\n",
    "        covariance_factor().\n",
    "        \"\"\"\n",
    "        self.factor = self.covariance_factor()\n",
    "        # Cache covariance and inverse covariance of the data\n",
    "        if not hasattr(self, '_data_inv_cov'):\n",
    "            # Compute the mean and residuals\n",
    "            _mean = np.sum(self.weights * self.dataset, axis=1)\n",
    "            _residual = (self.dataset - _mean[:, None])\n",
    "            # Compute the biased covariance\n",
    "            self._data_covariance = np.atleast_2d(np.dot(_residual * self.weights, _residual.T))\n",
    "            # Correct for bias (http://en.wikipedia.org/wiki/Weighted_arithmetic_mean#Weighted_sample_covariance)\n",
    "            self._data_covariance /= (1 - np.sum(self.weights ** 2))\n",
    "            self._data_inv_cov = np.linalg.inv(self._data_covariance)\n",
    "\n",
    "        self.covariance = self._data_covariance * self.factor**2\n",
    "        self.inv_cov = self._data_inv_cov / self.factor**2\n",
    "        self._norm_factor = np.sqrt(np.linalg.det(2*np.pi*self.covariance)) #* self.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Gaussian_kde_mod' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c69d2e21236c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mkde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussian_kde_mod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mkde\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_bandwidth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mkde\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkde\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Gaussian_kde_mod' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors.kde import KernelDensity\n",
    "\n",
    "train  = proj.transform(feat)\n",
    "test  = proj.transform(feat_test)\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=0.05).fit(train)\n",
    "\n",
    "#kde = Gaussian_kde_mod(train)\n",
    "kde.set_bandwidth()\n",
    "kde.fit(train)\n",
    "\n",
    "s = kde.score_samples(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = proj.transform(feat)\n",
    "test  = proj.transform(feat_test)\n",
    "\n",
    "weight = np.matmul(feat,w)-sig\n",
    "weight = np.sum(weight, axis =1)\n",
    "weight = weight-np.min(weight)\n",
    "weight = weight/np.sum(weight)\n",
    "\n",
    "kde = Gaussian_kde_mod(np.transpose(train), bw_method='scott', weights=weight*weight*weight*weight)\n",
    "s = np.zeros(test.shape[0])\n",
    "i = 0\n",
    "ends = 0\n",
    "while(ends<test.shape[0]):\n",
    "    start = i*1000\n",
    "    ends = min(start+1000, s.size)\n",
    "    s[start:ends] =  kde.evaluate(np.transpose(test[start:ends,:]))\n",
    "    i = i+1\n",
    "#s = kde.evaluate(np.transpose(test[:1000,:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1000\n",
      "1000 2000\n",
      "2000 3000\n",
      "3000 4000\n",
      "4000 5000\n",
      "5000 6000\n",
      "6000 7000\n",
      "7000 8000\n",
      "8000 9000\n",
      "9000 10000\n",
      "10000 11000\n",
      "11000 11282\n"
     ]
    }
   ],
   "source": [
    "train  = proj.transform(feat)\n",
    "test  = proj.transform(feat_test)\n",
    "\n",
    "\n",
    "kde = Gaussian_kde_mod(np.transpose(train[:,10:20]), bw_method='scott')\n",
    "s = np.zeros(test.shape[0])\n",
    "i = 0\n",
    "ends = 0\n",
    "while(ends<test.shape[0]):\n",
    "    start = i*1000\n",
    "    ends = min(start+1000, s.size)\n",
    "    print(start, ends)\n",
    "    s[start:ends] =  kde.evaluate(np.transpose(test[start:ends,:10]))\n",
    "    i = i+1\n",
    "#s = kde.evaluate(np.transpose(test[:1000,:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "s = s-np.min(s)\n",
    "s = s/np.mean(s)\n",
    "dirUtil.sortDirImages_topX('/media/daniel/D/Data/Flickr11K', -s, '/media/daniel/D/Data/tmp', 100,\n",
    "                           types=['*.jpg', '*.JPG', '*.png'], size=224, interp='bilinear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from highDimLearningX import estModel22\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "numIter =20\n",
    "mask = gt ==0\n",
    "pca = PCA(n_components=numIter)\n",
    "pca.fit(feat[mask,:])\n",
    "m = pca.components_\n",
    "sig = np.mean(np.matmul(feat[mask,:], np.transpose(m)), axis=0)\n",
    "w, sig = estModel22(feat[mask,:], np.transpose(m), sig, numShells=numIter, weight=0.01)\n",
    "score = np.matmul(feat[mask],w)-sig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.matmul(feat,w)-sig\n",
    "s = np.sum(score, axis =1)\n",
    "s = s-np.min(s)\n",
    "\n",
    "dirUtil.sortDirImages_topX('/media/daniel/D/Data/basicMITData/alley', -s, '/media/daniel/D/Data/tmp', 100,\n",
    "                           types=['*.jpg', '*.JPG', '*.png'], size=224, interp='bilinear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.matmul(feat_test,w)-sig\n",
    "s = np.sum(score, axis =1)\n",
    "s = s-np.min(s)\n",
    "\n",
    "\n",
    "dirUtil.sortDirImages_topX('/media/daniel/D/Data/Flickr11K', -s, '/media/daniel/D/Data/tmp', 100,\n",
    "                           types=['*.jpg', '*.JPG', '*.png'], size=224, interp='bilinear')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_train = np.matmul(feat[mask],w)-sig\n",
    "s_test = np.matmul(feat_test,w)-sig\n",
    "\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=0.01).fit(s_train)\n",
    "s = kde.score_samples(s_test)\n",
    "s = s-np.min(s)\n",
    "\n",
    "dirUtil.sortDirImages('/media/daniel/D/Data/Flickr11K', s, '/media/daniel/D/Data/tmp', types=['*.jpg', '*.JPG', '*.png'], size=224, interp='bilinear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "s = np.sum(score, axis =1)\n",
    "s = s-np.min(s)\n",
    "\n",
    "dirUtil.sortDirImages('/media/daniel/D/Data/basicMITData/abbey', s, '/media/daniel/D/Data/tmp', types=['*.jpg', '*.JPG', '*.png'], size=224, interp='bilinear')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
