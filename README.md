Shell theory hypothesizes the existence of naturally occurring class boundaries that are defined in terms of the mean of variance of each class.

Shell theory further suggests that these boundaries can be discovered (and hence classification achieved) by simply estimating the class mean of shell-normalized instances.

Shell-normalization is achieved by subtracting the mean of the test-data from each instance; then scaling the instance to be a unit-vector; this differs from “classical normalization” which is with respect to the mean of the training-data.

We show how shell theory can be employed on one-class svm and anomaly detection, with comparisons provided against one-class svm. 

Our experiments indicates that shell learning provides almost perfect results on such very difficult problems. 

Resnet50 features used are available at:
https://drive.google.com/file/d/1QZtdt5Nh_rXAtgP6CI0alOYwb6g8Z019/view?usp=sharing

Raw images are not strictly necessary for this code but you may want to play with them yourselves. They can be downloaded from:
https://drive.google.com/file/d/1bdBqmD9plsM2uLHAmon1n45mIZNCROrg/view?usp=sharing

Have fun!
Lin Wen-Yan, Daniel

By the way, if you found the code useful, could you please cite:
Lin, Daniel, et al. "Shell Theory: A Statistical Model of Reality." IEEE Transactions on Pattern Analysis and Machine Intelligence (2021).
